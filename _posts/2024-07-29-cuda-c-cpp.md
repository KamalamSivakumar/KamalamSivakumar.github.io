## parallel and distributed learning

Today, I am eager and invested to resume my journey into distributed and parallel computing. 
Reflecting on my college days at PSGCT, I remember how challenging this subject felt. 
My professor, who had previously worked at Intel, had rigorous lab sessions that often left me doubting my abilities. 
Despite my interest, my self-confidence was so low that I feared flunking the class, 
and I never fully embraced the learning opportunity.

Now, four years later, I'm ready to tackle this topic again with a fresh perspective. 
I realize that if I had the mindset I have today during my college days, 
I would have been more invested and interested. 
This renewed determination is my motivation to dive into CUDA and document my progress. (CUDA because my laptop has a GPU from NVIDIA)

Having said that, this is how I'm structuring my learning path.  
1. Foundational Knowledge: Basics of parallel computing | Difference between CPU and GPU architectures
2. Introduction to CUDA: What is CUDA? | CUDA architecture and terminology
3. CUDA Programming Basics
4. CUDA Libraries and Tools
5. Advanced CUDA Programming

I plan to write a post after completing each topic in the CUDA learning path. 
Resources: Medium articles, books, forums, and whatever legit information I could find from the internet. 
Will mention the resources with each post written.

It feels good to dive into something new while juggling a full-time job, and fittingly,  
Iâ€™m simultaneously learning about parallel computing in parallel!  
(I might have said this especially to make the pun. :P)
