## notes on llm fine-tuning; preference alignment methods: dpo, orpo

[part 1: rlhf](https://kamalamsivakumar.github.io/2024/08/08/understanding-fine-tuning-llms.html)

my notes about direct preference optimization (dpo), odds-ratio-preference-optimization (orpo)  

idea of advantage over the regular sft+rlhf:  
1. during the sft, the probability of generating undesirable responses along with preferred ones also increases, lacks preference sensitivity, and has no reward signals
2. it's expensive to perform rlhf (resource-intensive)

[dpo](https://arxiv.org/html/2305.18290v2)  
**direct preference optimization (dpo)**:  
**basic idea**:  
--skip the reward model and directly fine-tune the llm with the preference dataset. (sft + dpo instead of sft + rlhf)  
--basically combining the core concept of llms which is maximising the likelihood of a token and the preference dataset. i.e., llms itself work as a reward model and just giving it preference data would do the trick.  
--the **preference dataset** must be of the format: **prompt | preferred answer | rejected answer**  
 
![illustration](https://miro.medium.com/v2/resize:fit:720/format:webp/1*c_EO7XaHexRlASkqZRNDug.png)  

**identity preference optimization (ipo)**:  
--same idea as dpo, but with a regularization term to handle the overfitting  
--not proven to perform over dpo in every scenario, but this is an idea as well

[kto](https://contextual.ai/better-cheaper-faster-llm-alignment-with-kto/)  
**Kahneman-Tversky optimization (KTO)**:
really cool method.  
**basic idea**:  
--human opinions are biased, but the loss function that basically "rewards/penalties" the llm isn't aware of the biases.  
--this alignment method does not need preferred or rejected answers. just if the llm's response is desirable or not. (basically üëç or üëé)  
--makes data gathering really simple.  
--inspired from behavioural economics. really cool.  

[orpo](https://arxiv.org/pdf/2403.07691)  
**odds-ratio preference optimization**:  
--skip the sft+dpo with just orpo  
--modification in the standard language loss function by adding an odds-ratio term.  

![image](https://github.com/user-attachments/assets/7beb47ba-2858-49ba-a0fb-3078bbbc88fd)

--rewarding preferred answers and penalising rejected responses is the core idea.  
--the **preference data** must be of the format: **prompt | preferred answer | rejected answer/response**  

implementations for cuda, preference alignment will be uploaded. 
